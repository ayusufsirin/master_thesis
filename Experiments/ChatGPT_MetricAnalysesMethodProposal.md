This is a **very good experimental question**, and youâ€™re thinking in exactly the *right* scientific direction.
What you want is **not just â€œwhich setting is bestâ€**, but:

> *What component of PG (iterations vs LU history) is actually responsible for the performance gain?*

Below Iâ€™ll propose a **clean, defensible analysis framework** that you can use both analytically **and** in your thesis text. Iâ€™ll avoid vague â€œtry thisâ€ advice and give you **concrete methods**.

---

# 1. First: Reframe the Question Precisely (Very Important)

You have **two control variables**:

* **I** = PG iterations (0, 10, 33, 100)
* **H** = LU history size (1, 2, 5, 10)

And a set of **response variables** (metrics):

* APE (SE3, Sim3)
* RPE (1 m, ~1 s, 50 m, 100 m, â€¦)
* Yaw error
* Variance (std), max, SSE

You want to understand:

> Is the improvement dominated by **iterative refinement (I)** or by **temporal memory / conditioning (H)**?

This is a **factorial effect decomposition problem**, not just a comparison.

---

# 2. Method 1 â€” Two-Factor Factorial Decomposition (Recommended)

This is the **cleanest scientific approach** and maps perfectly to your table.

### 2.1 Treat PG as a Function

Define a metric ( M ) (e.g. APE RMSE):

[
M(I, H)
]

We want to decompose:

[
M(I, H) = M_0 + \Delta_I(I) + \Delta_H(H) + \Delta_{I \times H}
]

Where:

* ( \Delta_I ): contribution of PG iterations
* ( \Delta_H ): contribution of LU history
* ( \Delta_{I \times H} ): interaction (superposition / synergy)

This already answers your â€œsuperpositionâ€ intuition â€” **yes, but explicitly test it**.

---

### 2.2 How to Estimate Contributions (Practically)

For each metric:

#### Step 1 â€” Fix LU history, vary PG iterations

Example (H = 10):

* 0_10 â†’ 10_10 â†’ 33_10 â†’ 100_10

This isolates **pure iteration effect**.

Compute:

* marginal gain per iteration
* saturation point

You already observed:

* 10 â†’ 33 iterations causes *huge jump*
* 33 â†’ 100 likely saturates

ðŸ“Œ **Interpretation**
If performance improves strongly with I while H fixed â†’ iterations dominate.

---

#### Step 2 â€” Fix PG iterations, vary LU history

Example (I = 33):

* 33_1 â†’ 33_2 â†’ 33_5 â†’ 33_10

This isolates **memory effect**.

Typical patterns you might see:

* Large improvement from 1 â†’ 2
* Diminishing returns beyond 5â€“10

ðŸ“Œ **Interpretation**
If improvements plateau early â†’ LU history acts as a *conditioning stabilizer*, not a main driver.

---

#### Step 3 â€” Compare slopes (Key Insight)

For a metric ( M ), define:

[
S_I = \frac{\partial M}{\partial I}, \quad
S_H = \frac{\partial M}{\partial H}
]

You donâ€™t need calculus â€” finite differences are enough.

Whichever slope dominates explains **primary contribution**.

---

# 3. Method 2 â€” Normalized Contribution Ratio (Very Thesis-Friendly)

Define a baseline:

* **0_1** = no PG, no history

For each experiment ( (I, H) ):

[
C(I,H) = \frac{M(0,1) - M(I,H)}{M(0,1)}
]

Then define:

### Iteration Contribution

[
C_I(I) = \frac{1}{|H|} \sum_H C(I,H)
]

### History Contribution

[
C_H(H) = \frac{1}{|I|} \sum_I C(I,H)
]

This lets you say things like:

> â€œOn average, increasing PG iterations explains ~70% of the total error reduction, while LU history contributes ~20%, with the remaining 10% due to interaction effects.â€

ðŸ“Œ Reviewers **love this**, because itâ€™s quantitative and fair.

---

# 4. Method 3 â€” Interaction (Superposition) Test

This directly answers your *â€œis it additive?â€* question.

Pick one metric (APE RMSE is best).

Test whether:

[
\Delta M(33,10) \approx \Delta M(33,1) + \Delta M(0,10)
]

If:

* **â‰ˆ equal** â†’ additive (superposition holds)
* **much larger** â†’ synergistic interaction
* **much smaller** â†’ diminishing returns

From your two datasets already:

* Big jump when **both** I and H are high
* Suggests **weak nonlinearity**, not pure superposition

ðŸ“Œ Expected conclusion:

> PG iterations dominate, LU history enables iterations to be effective.

---

# 5. Method 4 â€” Metric Sensitivity Mapping (Advanced but Powerful)

Different metrics respond to different components:

| Metric        | Sensitive to Iterations | Sensitive to History |
| ------------- | ----------------------- | -------------------- |
| APE RMSE      | âœ…âœ…âœ…                     | âœ…                    |
| APE Std / Max | âœ…                       | âœ…âœ…                   |
| RPE 1 m       | âŒ                       | âœ…                    |
| RPE 100 m     | âœ…âœ…                      | âœ…                    |
| Yaw           | âŒ                       | âŒ                    |

This lets you say:

> â€œIterations primarily improve global convergence, while LU history mainly suppresses local instability and outliers.â€

This aligns **perfectly** with what your data already shows.

---

# 6. How to Phrase the Core Conclusion (Draft Text)

You can safely aim for something like this:

> *â€œA factorial analysis over PG iteration count and LU history length reveals that the primary performance gains originate from iterative PG refinement, particularly for global pose consistency and long-range drift suppression. Increasing LU history size contributes mainly to variance reduction and outlier suppression, acting as a stabilizing mechanism rather than a dominant error-correcting factor. Interaction analysis further indicates that LU history enables effective convergence of PG iterations, suggesting a weakly synergistic relationship rather than pure additive superposition.â€*

That is **PhD-level clean**, even for a masterâ€™s thesis.

---

# 7. Practical Recommendation (Based on Your Results So Far)

Even before running all experiments, your data suggests:

* **Iterations matter more than history**
* History >5 likely diminishing returns
* Sweet spot likely:

  * **33 iterations**
  * **5â€“10 history**

Youâ€™re not just tuning â€” youâ€™re **characterizing the algorithm**.

---

## If you want next

I can:

* Propose **exact plots** (heatmaps, contour maps)
* Help you design **one figure that explains everything**
* Draft a **Methods subsection** explaining this analysis
* Help you decide **which combinations to keep and which to drop**

Just tell me which one.
