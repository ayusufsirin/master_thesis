\section{Dinamik Papoulis-Gerchberg}  \label{sec:dynamic_pg}

Dinamik PG yapısı,

\subsection{Mathematical Formulation of the Depth Fusion Pipeline}

In this section, we formalize the mathematical model underlying our ROS--CuPy based ZED--LiDAR fusion node. The implementation operates on synchronized depth images from a stereo camera (ZED) and point clouds from a LiDAR sensor and produces a fused depth image and a corresponding point cloud in real time.

\begin{figure}[H]
    \centering
    \begin{adjustbox}{width=\textwidth}
        \includegraphics[width=\textwidth]{mermaid/pg.png}
    \end{adjustbox}
    \caption{Füzyon algoritması akış diyagramı 0}
    \label{methodology_2}
\end{figure}

\subsubsection*{Notation and Inputs}

Let the ZED depth image be
\begin{equation}
    D_{\text{ZED}}^{\text{orig}}(v,u) \in \mathbb{R} \cup \{\text{NaN}\},
\end{equation}
with pixel coordinates
\begin{equation}
    u \in \{0,\dots,W-1\}, \quad v \in \{0,\dots,H-1\}.
\end{equation}

The LiDAR returns a 3D point cloud
\begin{equation}
    \mathcal{P}_{\text{L}}
    =
    \left\{
        \mathbf{p}_k
    =
        \begin{bmatrix}
            x_k \\ y_k \\ z_k
        \end{bmatrix}
        \in \mathbb{R}^3
    \right\}_{k=1}^{N}.
\end{equation}

Camera intrinsics (from the ROS \texttt{CameraInfo} message) are given by
\begin{equation}
    f_x, f_y \quad \text{(focal lengths)}, \qquad
    c_x, c_y \quad \text{(principal point)}.
\end{equation}

We define a crop region
\begin{equation}
    \Omega_c = \left\{ (v,u) \;\middle|\;
                   t \le v < H - b,\
                   \ell \le u < W - r
    \right\},
\end{equation}
where
\begin{equation}
    t = \texttt{MORTAL\_ROWS\_TOP}, \quad
    b = \texttt{MORTAL\_ROWS\_BOTTOM}, \quad
    \ell = \texttt{MORTAL\_COLUMNS\_LEFT}, \quad
    r = \texttt{MORTAL\_COLUMNS\_RIGHT}.
\end{equation}
The consistency threshold between ZED and LiDAR depth is denoted by
\begin{equation}
    \tau = \texttt{ZED\_VLP\_DIFF\_MAX}.
\end{equation}

\subsubsection*{LiDAR Cartesian to Spherical Representation}

For each LiDAR point $\mathbf{p}_k = (x_k, y_k, z_k)^\top$, we define a spherical representation
\begin{align}
    r_k     &= \sqrt{x_k^2 + y_k^2 + z_k^2}, \\
    \theta_k &= \arctan\!\left( \frac{z_k}{\sqrt{x_k^2 + y_k^2}} \right), \\
    \phi_k   &= \arctan\!\left( \frac{y_k}{x_k} \right),
\end{align}
so that
\begin{equation}
    \mathbf{s}_k =
    \begin{bmatrix}
        r_k \\ \theta_k \\ \phi_k
    \end{bmatrix}.
\end{equation}
This representation is used for angular filtering and diagnostics in the implementation.

\subsubsection*{LiDAR to Camera Projection and Depth Image}

\paragraph{Axis Remapping.}
The LiDAR points are first remapped into a camera-like coordinate system via
\begin{equation}
    \begin{bmatrix}
        X_k \\ Y_k \\ Z_k
    \end{bmatrix}
    =
    R
    \begin{bmatrix}
        x_k \\ y_k \\ z_k
    \end{bmatrix},
    \qquad
    R =
    \begin{bmatrix}
        0 & -1 & 0 \\
        0 &  0 & -1 \\
        1 &  0 & 0
    \end{bmatrix},
\end{equation}
so that $Z_k$ is forward, $X_k$ is horizontal and $Y_k$ is vertical in the camera frame.

\paragraph{Pinhole Projection.}
Using the pinhole camera model, the projected image coordinates are
\begin{align}
    \tilde{u}_k &= \frac{X_k f_x}{Z_k} + c_x, \\
    \tilde{v}_k &= \frac{Y_k f_y}{Z_k} + c_y.
\end{align}
Discretization to pixel indices is performed by rounding:
\begin{equation}
    u_k = \operatorname{round}(\tilde{u}_k), \qquad
    v_k = \operatorname{round}(\tilde{v}_k).
\end{equation}

We only keep points that project to a valid pixel and lie in front of the camera:
\begin{equation}
    \mathcal{K}
    =
    \left\{ k \;\middle|\;
        0 \le u_k < W,\
    0 \le v_k < H,\
    Z_k > 0,\
    Z_k \text{ finite} \right\}.
\end{equation}

\paragraph{LiDAR Depth Image Construction.}
The LiDAR depth image $D_{\text{L}}(v,u)$ is defined as the minimum range $Z_k$ over all points that project to the corresponding pixel:
\begin{equation}
    D_{\text{L}}(v,u) =
    \begin{cases}
        \displaystyle
        \min\limits_{k \in \mathcal{K} : (v_k,u_k) = (v,u)} Z_k,
        & \text{if such } k \text{ exists}, \\[1.2ex]
        \text{NaN}, & \text{otherwise}.
    \end{cases}
\end{equation}
This corresponds to a nearest-surface depth image derived from the LiDAR point cloud.

\subsubsection*{ZED Depth Inpainting}

The raw ZED depth map $D_{\text{ZED}}^{\text{orig}}(v,u)$ may contain invalid values (NaN or $\pm\infty$). We define the invalidity mask
\begin{equation}
    M(v,u) =
    \begin{cases}
        1, & \text{if } D_{\text{ZED}}^{\text{orig}}(v,u) \text{ is NaN or } \pm\infty, \\
        0, & \text{otherwise}.
    \end{cases}
\end{equation}

We initialize an inpainted depth map by
\begin{equation}
    D^{(0)}(v,u) =
    \begin{cases}
        D_{\text{ZED}}^{\text{orig}}(v,u), & M(v,u) = 0, \\
        0, & M(v,u) = 1.
    \end{cases}
\end{equation}

Then we iteratively propagate valid neighbouring depths into invalid regions using a 4-connected neighbourhood
\begin{equation}
    \mathcal{N}(v,u) =
    \{(v-1,u), (v+1,u), (v,u-1), (v,u+1)\}.
\end{equation}
At iteration $t+1$, for each pixel with $M(v,u) = 1$, we update
\begin{equation}
    D^{(t+1)}(v,u)
    =
    \frac{
        \displaystyle
        \sum\limits_{(p,q) \in \mathcal{N}(v,u)}
        D^{(t)}(p,q)\,
        \mathbf{1}_{\{M(p,q) = 0\}}
    }{
        \displaystyle
        \sum\limits_{(p,q) \in \mathcal{N}(v,u)}
        \mathbf{1}_{\{M(p,q) = 0\}}
        + \varepsilon
    },
\end{equation}
where $\mathbf{1}_{\{\cdot\}}$ is the indicator function and $\varepsilon \ll 1$ is a small constant to prevent division by zero. Valid pixels ($M(v,u)=0$) remain unchanged in this iteration.

After a fixed number $T$ of iterations, we obtain the inpainted ZED depth map
\begin{equation}
    D_{\text{ZED}}(v,u) = D^{(T)}(v,u).
\end{equation}
The original invalid pixels are retained by reapplying the mask at the end.

\subsubsection*{Cropping and ZED--LiDAR Consistency Filtering}

We restrict the inpainted ZED depth and LiDAR depth to the crop region $\Omega_c$:
\begin{align}
    D_{\text{ZED}}^{c}(v,u) &= D_{\text{ZED}}(v,u),
    & (v,u) &\in \Omega_c, \\
    D_{\text{L}}^{c}(v,u)   &= D_{\text{L}}(v,u),
    & (v,u) &\in \Omega_c.
\end{align}

To reject inconsistent LiDAR measurements, we apply a consistency filter based on the ZED depth:
\begin{equation}
    D_{\text{L}}^{c}(v,u) \leftarrow
    \begin{cases}
        D_{\text{L}}^{c}(v,u),
        & \text{if } \left| D_{\text{ZED}}^{c}(v,u) - D_{\text{L}}^{c}(v,u) \right| \le \tau, \\[0.8ex]
        \text{NaN},
        & \text{otherwise},
    \end{cases}
    \qquad (v,u) \in \Omega_c.
\end{equation}

In the current implementation, the PG depth in the crop region is taken directly from the filtered LiDAR depth:
\begin{equation}
    D_{\text{PG}}^{c}(v,u) = D_{\text{L}}^{c}(v,u),
    \qquad (v,u) \in \Omega_c.
\end{equation}

\subsubsection*{Final Fused Depth Map}

The final fused depth map $D_{\text{PG}}(v,u)$, which is published as \texttt{PG\_DEPTH\_TOPIC}, is constructed as follows:

\begin{enumerate}
    \item Initialize with the original ZED depth:
    \begin{equation}
        D_{\text{PG}}(v,u) \leftarrow D_{\text{ZED}}^{\text{orig}}(v,u).
    \end{equation}

    \item In the crop region, overwrite with LiDAR depth wherever it is valid:
    \begin{equation}
        D_{\text{PG}}(v,u) =
        \begin{cases}
            D_{\text{L}}^{c}(v,u),
            & (v,u) \in \Omega_c,\ D_{\text{L}}^{c}(v,u) \text{ finite}, \\[0.8ex]
            D_{\text{ZED}}^{\text{orig}}(v,u),
            & \text{otherwise}.
        \end{cases}
    \end{equation}

    \item Enforce the original ZED invalidity mask:
    \begin{equation}
        D_{\text{PG}}(v,u) \leftarrow \text{NaN}
        \quad \text{for all } (v,u) \text{ such that } M(v,u) = 1.
    \end{equation}
\end{enumerate}

Combining these steps, we can concisely express the fused depth as
\begin{equation}
    D_{\text{PG}}(v,u) =
    \begin{cases}
        \text{NaN},
        & M(v,u) = 1, \\[0.8ex]
        D_{\text{L}}^{c}(v,u),
        & (v,u) \in \Omega_c,\ D_{\text{L}}^{c}(v,u) \text{ finite}, \\[0.8ex]
        D_{\text{ZED}}^{\text{orig}}(v,u),
        & \text{otherwise}.
    \end{cases}
\end{equation}

\subsubsection*{Back-Projection to 3D and Output Point Cloud}

The fused depth map $D_{\text{PG}}(v,u)$ is converted back to 3D using the camera intrinsics. For each pixel $(v,u)$ with a finite depth
\begin{equation}
    d = D_{\text{PG}}(v,u),
\end{equation}
the point in the camera frame is
\begin{align}
    X_{\text{cam}}(v,u) &= \frac{(u - c_x)\, d}{f_x}, \\
    Y_{\text{cam}}(v,u) &= \frac{(v - c_y)\, d}{f_y}, \\
    Z_{\text{cam}}(v,u) &= d.
\end{align}

The implementation applies an axis remapping so that the output frame uses
\begin{align}
    x'(v,u) &= Z_{\text{cam}}(v,u) = d, \\
    y'(v,u) &= -X_{\text{cam}}(v,u) = -\frac{(u - c_x)\, d}{f_x}, \\
    z'(v,u) &= -Y_{\text{cam}}(v,u) = -\frac{(v - c_y)\, d}{f_y}.
\end{align}
Thus, the resulting 3D point corresponding to pixel $(v,u)$ is
\begin{equation}
    \mathbf{p}'(v,u)
    =
    \begin{bmatrix}
        x'(v,u) \\
        y'(v,u) \\
        z'(v,u)
    \end{bmatrix}
    =
    \begin{bmatrix}
        d \\[0.4ex]
        -\dfrac{(u - c_x)\, d}{f_x} \\[1.0ex]
        -\dfrac{(v - c_y)\, d}{f_y}
    \end{bmatrix}.
\end{equation}

Collecting all valid pixels yields the fused point cloud
\begin{equation}
    \mathcal{P}_{\text{PG}}
    =
    \left\{
        \mathbf{p}'(v,u)
        \;\middle|\;
        D_{\text{PG}}(v,u) \text{ is finite}
    \right\},
\end{equation}
which is serialized as a \texttt{sensor\_msgs/PointCloud2} message and published on the fused point cloud topic.

\subsection{Sentetik Veri ile Algoritmanın Doğrulanması}
Gerçek sensör verisiyle yapılan iyileştirme denemelerinde gözlemlenen artefaktların (özellikle ``spike'' benzeri geometrilerin ve yakınsama kararsızlıklarının) kaynağını daha net ayırt edebilmek amacıyla, algoritmanın doğrulama aşamasında kontrollü bir sentetik veri seti kullanılmıştır. Bu doğrulama yaklaşımının temel motivasyonu; sensör gürültüsü, görüş alanı (FOV) uyuşmazlığı, eşzamanlılık sapmaları, hareket kaynaklı bozulmalar ve eksik örneklem gibi gerçek dünya etkilerini geçici olarak devre dışı bırakarak, Papoulis--Gerchberg (PG) tabanlı iyileştirme sürecinin ``beklenen'' davranışını gözlemleyebilmektir.

Bu kapsamda dama tahtası (satranç tahtası) formunda üretilmiş sentetik desen üzerinden iki ayrı kaynak veri hazırlanmıştır: (i) ZED stereo kamera temsili nokta/derinlik verisi ve (ii) VLP-16 LiDAR temsili nokta bulutu. Şekil~\ref{fig:synthetic_validation_triplet} içerisinde sırasıyla ZED temsili veri (Şekil~\ref{fig:dama_zed}), VLP temsili veri (Şekil~\ref{fig:dama_vlp}) ve PG çıktısı (Şekil~\ref{fig:dama_pg}) birlikte sunulmuştur. Bu deneyde, desenin yüksek kontrastlı ve düzenli geometrik yapısı sayesinde, PG algoritmasının frekans alanındaki kısıtlama ve uzamsal alandaki yeniden yapılandırma adımlarının ürettiği etki kolayca izlenebilmiştir.

\begin{figure}[H]
    \centering

    \begin{subfigure}[t]{0.6\textwidth}
        \centering
        \includegraphics[width=\linewidth]{./Figures/pg_synthetic_validation/dama_zed.png}
        \caption{ZED stereo kameradan elde edilen sentetik dama tahtası verisi}
        \label{fig:dama_zed}
    \end{subfigure}

    \vspace{0.6em}

    \begin{subfigure}[t]{0.6\textwidth}
        \centering
        \includegraphics[width=\linewidth]{./Figures/pg_synthetic_validation/dama_vlp.png}
        \caption{VLP-16 LiDAR’dan elde edilen sentetik dama tahtası verisi}
        \label{fig:dama_vlp}
    \end{subfigure}

    \vspace{0.6em}

    \begin{subfigure}[t]{0.6\textwidth}
        \centering
        \includegraphics[width=\linewidth]{./Figures/pg_synthetic_validation/dama_pg.png}
        \caption{Papoulis--Gerchberg (PG) algoritması sonrası elde edilen çıktı}
        \label{fig:dama_pg}
    \end{subfigure}

    \caption{Sentetik dama tahtası verisi ile ZED, VLP-16 ve PG çıktılarının karşılaştırılması. Bu deney, gerçek veri kaynaklı belirsizliklerden bağımsız olarak algoritmanın temel yakınsama davranışını doğrulamak amacıyla gerçekleştirilmiştir.}
    \label{fig:synthetic_validation_triplet}
\end{figure}

Görsellerde görüldüğü üzere LiDAR verisine karşılık üretilen sentetik veri satranç tahtası üzerinde ölçüm hassasiyeti çok yüksek ancak seyrek noktalardan oluşmaktadır. Benzer mantıkla stereo sentetik verisi ise yüksek çözünürlüklü ancak gürültülüdür.

Elde edilen sonuçlar, algoritmanın sentetik veri üzerinde kararlı bir yakınsama sergilediğini ve iki veri kaynağı arasında beklenen yönde bir uyum üretebildiğini göstermektedir. Ayrıca danışman değerlendirmesi kapsamında, elde edilen çıktının görsel olarak tutarlı bulunduğu ve algoritmanın temel çalışma prensibinin doğrulandığı not edilmiştir. Bu bulgu, gerçek veri ile benzer bir görsel çıktının elde edilememesi durumunda problemin doğrudan algoritmanın çekirdek mantığından ziyade, gerçek sensör koşullarına (özellikle ZED ve VLP-16 görüş alanlarının PG sürecinde etkin kullanılan ortak FOV ile tam örtüşmemesi, senkronizasyon/örnekleme farklılıkları ve tarihsel LiDAR birikiminin gölgeleme etkileri) bağlı olabileceğine işaret etmektedir.

Dolayısıyla bu doğrulama adımı, çalışmanın devamında gerçekleştirilen filtre tipi seçimi ve parametre ayarlama sürecine metodolojik bir temel sağlamaktadır. Sentetik veri deneyinde algoritmanın ``ideal'' koşullarda çalıştığının gösterilmesi sayesinde, bir sonraki aşamada ele alınan filtre seçimi (Brick-wall, Gaussian, Butterworth) ve cutoff/iterasyon gibi parametrelerin belirlenmesi, gerçek veride ortaya çıkan artefaktları azaltmaya ve yakınsamayı daha kararlı hâle getirmeye yönelik sistematik bir optimizasyon problemi olarak ele alınabilmiştir. Bu nedenle, sentetik doğrulama sonrasında Bölüm~\ref{sec:pg_filter_selection} ve Bölüm~\ref{sec:pg_parameter_tuning} altında sunulan filtre/parametre analizlerine geçilmiştir.

\subsection{3B İçin Filtre Seçimi}

Bu çalışmada, LiDAR (VLP-16) ve stereo kamera (ZED) verilerinin birlikte kullanıldığı üç boyutlu veri füzyonu sürecinde, Papoulis--Gerchberg (PG) tabanlı iyileştirme algoritmasının filtreleme parametrelerine olan duyarlılığı incelenmiştir. PG algoritması, eksik veya örtüşmeyen frekans bileşenlerinin iteratif olarak yeniden yapılandırılmasına dayandığından, özellikle frekans alanında uygulanan filtreleme işlemleri, sonuç geometrisi üzerinde belirleyici bir etkiye sahiptir.

Gerçek veri üzerinde yapılan deneylerde, sensörlerin anlık olarak algılayamadığı ancak zamansal olarak biriktirilmiş (historical) LiDAR noktalarının, PG algoritmasını fiziksel olarak anlamlı olmayan yakınsama davranışlarına zorladığı gözlemlenmiştir. Bu durum, düşük geçiren filtrelerin (Low-Pass Filter, LPF) yanlış yapılandırılması halinde, uzaysal alanda orijine doğru uzanan ve “spike” benzeri yapay geometrilerin ortaya çıkmasına neden olmaktadır. Bu bağlamda, filtreleme süreci yalnızca gürültü bastırma amacıyla değil, aynı zamanda sensörler arası tutarsızlıkların etkisini sınırlamak için ele alınmıştır.

\subsubsection{Cutoff Frekansını Ayarlama}

Filtreleme sürecinde kullanılan cutoff frekansı, Papoulis--Gerchberg algoritmasının yakınsama karakteristiğini doğrudan etkilemektedir. Bu çalışmada, farklı normalize cutoff frekans değerleri için (0.01--0.16 aralığında) sabit iterasyon sayısı altında elde edilen çıktılar karşılaştırılmıştır. Düşük cutoff değerlerinde, yüksek frekans bileşenlerinin büyük ölçüde bastırıldığı ve bunun sonucunda özellikle ince geometrik detayların kaybolduğu gözlemlenmiştir. Buna karşılık, yüksek cutoff değerlerinde ise sensörler arası uyumsuzluklardan kaynaklanan yüksek frekanslı bileşenlerin yeterince bastırılamadığı görülmüştür.

Sentetik satranç tahtası (checkerboard) verisi üzerinde gerçekleştirilen deneyler (Şekil~\ref{fig:pg_cutoff_comparison}), cutoff frekansının uygun seçilmesi halinde PG algoritmasının teorik beklentilerle uyumlu şekilde çalıştığını doğrulamıştır. Ancak gerçek veri üzerinde, ZED kamera ve VLP LiDAR sensörlerinin görüş alanlarının (Field of View, FOV) tam olarak örtüşmemesi nedeniyle, aynı cutoff değerlerinin her durumda benzer sonuçlar üretmediği tespit edilmiştir. Bu durum, cutoff frekansının sabit bir parametre olarak değil, sensör geometrisi ve veri dağılımına bağlı olarak değerlendirilmesi gerektiğini göstermektedir.

\begin{figure}[H]
    \centering

    % -------- Row 1 --------
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[height=6cm]{./Figures/pg_filter_selection/dama_0.01_100.png}
        \caption{Cutoff = 0.01, Iter = 100}
        \label{fig:pg_001}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[height=6cm]{./Figures/pg_filter_selection/dama_0.02_100.png}
        \caption{Cutoff = 0.02, Iter = 100}
        \label{fig:pg_002}
    \end{subfigure}

    \vspace{0.8em}

    % -------- Row 2 --------
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[height=6cm]{./Figures/pg_filter_selection/dama_0.04_100.png}
        \caption{Cutoff = 0.04, Iter = 100}
        \label{fig:pg_004}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[height=6cm]{./Figures/pg_filter_selection/dama_0.08_100.png}
        \caption{Cutoff = 0.08, Iter = 100}
        \label{fig:pg_008}
    \end{subfigure}

    \vspace{0.8em}

    % -------- Row 3 --------
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[height=6cm]{./Figures/pg_filter_selection/dama_0.16_100.png}
        \caption{Cutoff = 0.16, Iter = 100}
        \label{fig:pg_016}
    \end{subfigure}

    \caption{Papoulis--Gerchberg algoritmasında farklı normalize cutoff frekansları için elde edilen çıktılar.
    Düşük cutoff değerlerinde yüksek frekans bileşenlerinin baskılandığı, yüksek cutoff değerlerinde ise detayların
    daha belirgin hâle geldiği gözlemlenmektedir.}
    \label{fig:pg_cutoff_comparison}
\end{figure}

Ayrıca, derinlik imajlarının kare olmayan boyutlara sahip olması durumunda, frekans alanında kullanılan dairesel maskelemenin hatalı sonuçlar ürettiği gözlemlenmiştir. Bu problem, eliptik maske kullanımı ile giderilmiş ve dikdörtgen ve kare imajlar arasında tutarlı sonuçlar elde edilmiştir.

\subsubsection{Filtre Tipi Seçimi}

Varsayılan olarak kullanılan brick-wall (ideal) düşük geçiren filtre, frekans alanında keskin bir kesim uygulaması nedeniyle uzaysal alanda salınımlara (ringing artefaktları) yol açmaktadır. Bu durum, özellikle sensörler arası mesafenin fazla olduğu bölgelerde, PG çıktısında yapay dalgalanmalara neden olmaktadır. Bu nedenle, çalışmada brick-wall filtreye alternatif olarak Gaussian ve Butterworth filtreler değerlendirilmiştir.

Farklı filtre tiplerinin aynı parametrelerdeki çıktıları Şekil~\ref{fig:pg_filter_comparison} içinde birlikte sunulmuştur. Gaussian filtre (Şekil~\ref{fig:pg_gaussian}), frekans alanında yumuşak bir geçiş sağladığından, yüksek frekans bileşenlerini kademeli olarak bastırmakta ve uzaysal alanda daha kararlı sonuçlar üretmektedir. Butterworth filtre (Şekil~\ref{fig:pg_butterworth}) ise geçiş bölgesinin eğimini kontrol edilebilir kılması sayesinde, brick-wall filtreden (Şekil~\ref{fig:pg_brickwall}) daha yumuşak, Gaussian filtreden ise daha seçici bir yapı sunmaktadır. Yapılan deneyler, her iki filtrenin de brick-wall filtreye kıyasla, özellikle spike oluşumlarını belirgin şekilde azalttığını göstermiştir.

Bununla birlikte, Gaussian ve Butterworth filtrelerin kullanımı sonucunda, zemin düzlemi gibi LiDAR verisinin yoğun olduğu bölgelerde bozulmalar meydana geldiği gözlemlenmiştir. Bu durum, filtreleme işleminin tüm sahneye homojen biçimde uygulanmasının her zaman ideal olmadığını ve baskın geometrik yapıların ayrı ele alınması gerektiğini ortaya koymaktadır.

\begin{figure}[H]
    \centering

    % ---------- Row 1 ----------
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[height=7cm]{./Figures/pg_filter_selection/dama_gaussian_0.64_2000.png}
        \caption{Gaussian LPF çıktısı}
        \label{fig:pg_gaussian}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[height=7cm]{./Figures/pg_filter_selection/dama_butterworth_0.64_2000.png}
        \caption{Butterworth LPF çıktısı}
        \label{fig:pg_butterworth}
    \end{subfigure}

    \vspace{0.8em}

    % ---------- Row 2 ----------
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[height=7cm]{./Figures/pg_filter_selection/dama_brick-wall_0.64_2000.png}
        \caption{Brick-wall LPF çıktısı}
        \label{fig:pg_brickwall}
    \end{subfigure}

    \vspace{0.8em}

    % ---------- Row 3 ----------
    \begin{subfigure}[t]{\textwidth}
        \centering

        \renewcommand{\arraystretch}{1.15}
        \begin{tabular}{lccc}
            \toprule
            \textbf{Filtre Tipi} & \textbf{Cutoff} & \textbf{Iterasyon} & \textbf{Notlar}            \\
            \midrule
            Gaussian             & 0.64            & 2000               & Yumuşak geçiş, düşük spike \\
            Butterworth          & 0.64            & 2000               & Dengeli yapı               \\
            Brick-wall           & 0.64            & 2000               & Keskin geçiş, artefakt     \\
            \bottomrule
        \end{tabular}

        \caption{Filtre parametreleri}
        \label{fig:pg_filter_table}
    \end{subfigure}

    \caption{Papoulis--Gerchberg algoritması için farklı düşük geçiren filtrelerin karşılaştırılması.
    Sarı noktalar ZED, yeşil noktalar VLP, uzaklığa göre geçişli renklerle gösterilen noktalar füzyon çıktısı verileri temsil etmektedir.}
    \label{fig:pg_filter_comparison}
\end{figure}

\subsubsection{Filtrelerin Çapraz Karşılaştırılması}

Filtre tiplerinin karşılaştırmalı analizi hem sentetik hem de gerçek veri üzerinde gerçekleştirilmiştir. Sentetik veride, ideal koşullar altında filtrelerin benzer yakınsama davranışı sergilediği görülürken, gerçek veri üzerinde belirgin farklar ortaya çıkmıştır. Brick-wall filtre, frekans alanında keskin bir kesim uyguladığı için PG çıktısında artefaktlara ve yer yer kararsız yakınsamaya neden olmuştur. Bu çalışmada tercih edilen Gaussian filtre ise yumuşak geçiş karakteristiği sayesinde sensörler arası uyumsuzlukları daha iyi tolere etmiş ve görsel olarak daha kararlı, daha tutarlı bir iyileştirme sağlamıştır. Butterworth filtre belirli senaryolarda dengeli bir alternatif sunsa da, yapılan deneylerde en tutarlı sonuçlar Gaussian filtre ile elde edilmiştir (Bakınız Şekil~\ref{fig:pg_gaussian_vs_brickwall_citrus}).

\begin{figure}[H]
    \centering

    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[height=5.5cm]{./Figures/pg_filter_selection/pg_nan_filled_1.png}
        \caption{Brick-wall LPF ile elde edilen PG çıktısı (Kırmızı noktalar PG, sarı noktalar ZED, yeşil noktalar VLP çıktısı)}
        \label{fig:pg_brickwall_not_variance_filtered}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[height=5.5cm]{./Figures/pg_filter_selection/variance_filtered.png}
        \caption{Brick-wall LPF ve Varyans Filtresi ile elde edilen PG çıktısı (Kırmızı noktalar PG, sarı noktalar ZED, yeşil noktalar VLP çıktısı)}
        \label{fig:pg_brickwall_variance_filtered}
    \end{subfigure}

    \vspace{0.8em}

    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.5\linewidth]{./Figures/pg_filter_selection/citrus_gaussian_0.16_33.png}
        \caption{Gaussian LPF ve Varyans Filtresi ile elde edilen PG çıktısı (Beyaz noktalar ZED, kırmızı noktalar VLP, gökkuşağı noktalar PG çıktısı)}
        \label{fig:pg_gaussian_citrus}
    \end{subfigure}

    \caption{Citrus Farm verisinde benzer frame’ler üzerinde Papoulis--Gerchberg (PG) çıktılarının karşılaştırılması.
    Brick-wall düşük geçiren filtre kullanıldığında düzeltme süreci daha belirgin artefaktlar (örn. küçük spike yapıları)
    üretebilirken, Gaussian filtre kullanımı daha yumuşak geçiş sağlayarak yakınsamanın görsel kararlılığını artırmıştır.
    Bu nedenle, ilgili deney setinde Gaussian filtre, Brick-wall filtreye kıyasla daha tutarlı bir PG iyileştirmesi sunmuştur.}
    \label{fig:pg_gaussian_vs_brickwall_citrus}
\end{figure}

Ayrıca, ZED ve VLP referans çerçeveleri arasında hesaplanan varyans (covariance) temelli maskeleme yaklaşımı, filtreleme süreciyle birlikte kullanıldığında, yüksek varyanslı LiDAR noktalarının PG güncellemesine dahil edilmemesi sayesinde daha kararlı sonuçlar elde edilmesini sağlamıştır. Bu yaklaşım, algoritmanın zorlayıcı ve fiziksel olarak anlamsız düzeltmeler yapması yerine, yalnızca güvenilir bölgelerde iyileştirme gerçekleştirmesine olanak tanımaktadır.

Sonuç olarak, bu çalışmada elde edilen bulgular, üç boyutlu veri füzyonunda filtre seçiminin yalnızca matematiksel değil, aynı zamanda fiziksel sahne yapısı ve sensör karakteristikleriyle birlikte değerlendirilmesi gerektiğini göstermektedir. Filtre tipi, cutoff frekansı ve maskeleme stratejilerinin birlikte ele alındığı bir yapı, Papoulis--Gerchberg tabanlı iyileştirme algoritmasının gerçek veri üzerindeki başarımını anlamlı ölçüde artırmaktadır.
