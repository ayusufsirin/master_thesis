@misc{nasa_lidar,
    author = {E. V. {Ashburn} and F. {Conner} and W. W. {Hildreth} Jr. and H. A. {Thorpe}},
    title = {Study of high resolution wind measuring systems. phase ii- analysis},
    year = {1965},
    url = {https://ntrs.nasa.gov/api/citations/19650024807},
    institution = {NASA},
    howpublished = {NASA Technical Report, Report Number: NASA-CR-67138},
}

@manual{jetson_nano,
    title = {NVIDIA Jetson Nano Developer Kit},
    author = {NVIDIA Corporation},
    year = {2019},
    url = {https://developer.nvidia.com/embedded/jetson-nano-developer-kit},
    note = {Accessed: 2024-12-18}
}

@manual{jetson_xavier_nx,
    title = {NVIDIA Jetson Xavier NX Developer Kit},
    author = {NVIDIA Corporation},
    year = {2020},
    url = {https://developer.nvidia.com/embedded/jetson-xavier-nx},
    note = {Accessed: 2024-12-18}
}

@manual{jetson_orin_nx,
    title = {NVIDIA Jetson Orin NX Developer Kit},
    author = {NVIDIA Corporation},
    year = {2022},
    url = {https://developer.nvidia.com/embedded/jetson-orin-nx},
    note = {Accessed: 2024-12-18}
}

@manual{zed_camera,
    title = {ZED Stereo Camera},
    author = {Stereolabs},
    year = {2015},
    url = {https://www.stereolabs.com/zed/},
    note = {Accessed: 2024-12-18}
}

@manual{zed2_camera,
    title = {ZED2 Stereo Camera},
    author = {Stereolabs},
    year = {2019},
    url = {https://www.stereolabs.com/zed-2/},
    note = {Accessed: 2024-12-18}
}

@ARTICLE{dataset_ComplexUrban,
    AUTHOR = { Jinyong Jeong and Younggun Cho and Young-Sik Shin and Hyunchul Roh and Ayoung Kim },
    TITLE = { Complex Urban Dataset with Multi-level Sensors from Highly Diverse Urban Environments },
    JOURNAL = { International Journal of Robotics Research },
    YEAR = { 2019 },
    VOLUME = { 38 },
    NUMBER = { 6 },
    PAGES = { 642--657 },
}

@inproceedings{dataset_groundchallenge,
    title = {Ground-challenge: A multi-sensor slam dataset focusing on corner cases for ground robots},
    author = {Yin, Jie and Yin, Hao and Liang, Conghui and Jiang, Haitao and Zhang, Zhengyou},
    booktitle = {2023 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
    pages = {1--5},
    year = {2023},
    organization = {IEEE}
}

@inproceedings{dataset_conslam,
    title = {Conslam: Periodically collected real-world construction dataset for SLAM and progress monitoring},
    author = {Trzeciak, Maciej and Pluta, Kacper and Fathy, Yasmin and Alcalde, Lucio and Chee, Stanley and Bromley, Antony and Brilakis, Ioannis and Alliez, Pierre},
    booktitle = {Computer Vision--ECCV 2022 Workshops: Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part VII},
    pages = {317--331},
    year = {2023},
    organization = {Springer}
}

@inproceedings{dataset_wildplaces,
    author = {Joshua Knights and Kavisha Vidanapathirana and Milad Ramezani and Sridha Sridharan and Clinton Fookes and Peyman Moghadam},
    title = {Wild-Places: A Large-Scale Dataset for Lidar Place Recognition in Unstructured Natural Environments},
    booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
    year = {2023},
}

@manual{velodyne_vlp16,
    author = {Velodyne Lidar Inc.},
    title = {VLP-16 User Manual},
    year = {2016},
    url = {https://velodynelidar.com/products/puck/},
    note = {Accessed: 2024-12-18},
    organization = {Velodyne Lidar Inc.},
}

@manual{arnav_manual,
    title = {ArNav S1G User Manual},
    author = {Ardic Research and Development Corporation},
    year = {2024},
    note = {Document Revision 1.0.13},
    url = {https://www.ardiclabs.com},
    publisher = {Ardic Research and Development Corporation, Middle East Technical University Technopolis, Ankara, Turkey}
}

@manual{mti7_manual,
    title = {MTi-7 Datasheet},
    author = {Xsens Technologies B.V.},
    year = {2023},
    note = {Document Revision May 2023},
    url = {https://www.xsens.com},
    publisher = {Xsens Technologies B.V., Enschede, The Netherlands}
}

@misc{zed_ros_wrapper,
    author = {Stereolabs},
    title = {ZED ROS Wrapper},
    year = {2025},
    url = {https://github.com/stereolabs/zed-ros-wrapper},
    note = {Accessed: 2025-01-12}
}

@misc{velodyne_ros_drivers,
    author = {ROS Drivers Maintainers},
    title = {Velodyne ROS Drivers},
    year = {2025},
    url = {https://github.com/ros-drivers/velodyne},
    note = {Accessed: 2025-01-12}
}

@inproceedings{robot_localization,
    author = {T. Moore and D. Stouch},
    title = {A Generalized Extended Kalman Filter Implementation for the Robot Operating System},
    year = {2014},
    month = {July},
    booktitle = {Proceedings of the 13th International Conference on Intelligent Autonomous Systems (IAS-13)},
    publisher = {Springer}
}

@misc{evo,
    title = {evo: Python package for the evaluation of odometry and SLAM.},
    author = {Grupp, Michael},
    howpublished = {\url{https://github.com/MichaelGrupp/evo}},
    year = {2017}
}

@article{umeyama,
    title = {Least-squares estimation of transformation parameters between two point patterns},
    author = {Umeyama, Shinji},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    volume = {13},
    number = {4},
    pages = {376--380},
    year = {1991},
    publisher = {IEEE}
}

@INPROCEEDINGS{pg_konferans,
    author = {Özbay, Bengisu and Kuzucu, Elvan and Gül, Mustafa and Öztürk, Dilan and Taşcı, Muhittin and Arısoy, A. Mansur and Şirin, Halil Onur and Uyanık, İsmail},
    booktitle = {2015 International Conference on Advanced Robotics (ICAR)},
    title = {A high frequency 3D LiDAR with enhanced measurement density via Papoulis-Gerchberg},
    year = {2015},
    volume = {},
    number = {},
    pages = {543-548},
    keywords = {Three-dimensional displays;Laser radar;Frequency measurement;Robot sensing systems;Image resolution;Density measurement;Delays},
    doi = {10.1109/ICAR.2015.7251509} }

% Verified

@INPROCEEDINGS{scharstein2001taxonomy,
    author = {Scharstein, D. and Szeliski, R. and Zabih, R.},
    booktitle = {Proceedings IEEE Workshop on Stereo and Multi-Baseline Vision (SMBV 2001)},
    title = {A taxonomy and evaluation of dense two-frame stereo correspondence algorithms},
    year = {2001},
    volume = {},
    number = {},
    pages = {131-140},
    keywords = {Taxonomy;Bismuth},
    doi = {10.1109/SMBV.2001.988771}
}

% Verified

@misc{zbontar2016stereomatchingtrainingconvolutional,
    title = {Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches},
    author = {Jure Žbontar and Yann LeCun},
    year = {2016},
    eprint = {1510.05970},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV},
    url = {https://arxiv.org/abs/1510.05970},
}

% Verified

@article{laga2022survey,
    title = "A Survey on Deep Learning Techniques for Stereo-based Depth Estimation",
    abstract = "Estimating depth from RGB images is a long-standing ill-posed problem, which has been explored for decades by the computer vision, graphics, and machine learning communities. Among the existing techniques, stereo matching remains one of the most widely used in the literature due to its strong connection to the human binocular system. Traditionally, stereo-based depth estimation has been addressed through matching hand-crafted features across multiple images. Despite the extensive amount of research, these traditional techniques still suffer in the presence of highly textured areas, large uniform regions, and occlusions. Motivated by their growing success in solving various 2D and 3D vision problems, deep learning for stereo-based depth estimation has attracted growing interest from the community, with more than 150 papers published in this area between 2014 and 2019. This new generation of methods has demonstrated a significant leap in performance, enabling applications such as autonomous driving and augmented reality. In this article, we provide a comprehensive survey of this new and continuously growing field of research, summarize the commonly used pipelines, and discuss their benefits and limitations. In retrospect of what has been achieved so far, we conjecture what the future may hold for deep learning-based stereo for depth estimation research.",
    author = "H. Laga and Jospin, \{L. V.\} and F. Boussaid and M. Bennamoun",
    year = "2022",
    month = apr,
    day = "1",
    doi = "10.1109/TPAMI.2020.3032602",
    language = "English",
    volume = "44",
    pages = "1738--1764",
    journal = "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    issn = "0162-8828",
    publisher = "IEEE, Institute of Electrical and Electronics Engineers",
    number = "4",
}

% Verified

@article{poggi2021synergies,
    author = {Matteo Poggi and
 Fabio Tosi and
 Konstantinos Batsos and
 Philippos Mordohai and
 Stefano Mattoccia},
    title = {On the Synergies between Machine Learning and Stereo: a Survey},
    journal = {CoRR},
    volume = {abs/2004.08566},
    year = {2020},
    url = {https://arxiv.org/abs/2004.08566},
    eprinttype = {arXiv},
    eprint = {2004.08566},
    timestamp = {Wed, 22 Apr 2020 12:57:53 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-2004-08566.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    doi = {10.1109/TPAMI.2021.3070917},
}

% Verified

@article{fastfusion,
    author = {Meng, Haitao and Li, Changcai and Zhong, Chonghao and Gu, Jianfeng and Chen, Gang and Knoll, Alois},
    title = {FastFusion: Deep stereo-LiDAR fusion for real-time high-precision dense depth sensing},
    journal = {Journal of Field Robotics},
    volume = {40},
    number = {7},
    pages = {1804-1816},
    keywords = {artificial intelligence, depth estimation},
    doi = {https://doi.org/10.1002/rob.22216},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.22216},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.22216},
    abstract = {Abstract Light detection and ranging (LiDAR) and stereo cameras are two generally used solutions for perceiving 3D information. The complementary properties of these two sensor modalities motivate a fusion to derive practicable depth sensing toward real-world applications. Promoted by deep neural network (DNN) techniques, recent works achieve superior performance on accuracy. However, the complex architecture and the sheer number of DNN parameters often lead to poor generalization capacity and non-real-time computing. In this paper, we present FastFusion, a three-stage stereo-LiDAR deep fusion scheme, which integrates the LiDAR priors into each step of classical stereo-matching taxonomy, gaining high-precision dense depth sensing in a real-time manner. We integrate stereo-LiDAR information by taking advantage of a compact binary neural network and utilize the proposed cross-based LiDAR trust aggregation to further fuse the sparse LiDAR measurements in the back-end of stereo matching. To align the photometrical of the input image and the depth of the estimation, we introduce a refinement network to guarantee consistency. More importantly, we present a graphic processing unit-based acceleration framework for providing a low-latency implementation of FastFusion, gaining both accuracy improvement and real-time responsiveness. In the experiments, we demonstrate the effectiveness and practicability of FastFusion, which obtains a significant speedup over state-of-the-art baselines while achieving comparable accuracy on depth sensing. The video demo for real-time depth estimation of FastFusion on the real-world driving scenario is available at https://youtu.be/nP7cls2BA8s.},
    year = {2023}
}

% Verified

@inproceedings{park2018stereolidar,
    author = {Park, Kihong and Kim, Seungryong and Sohn, Kwanghoon},
    booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
    title = {High-Precision Depth Estimation with the 3D LiDAR and Stereo Fusion},
    year = {2018},
    volume = {},
    number = {},
    pages = {2156-2163},
    keywords = {Three-dimensional displays;Laser radar;Estimation;Computer architecture;Sensors;Reliability;Image color analysis},
    doi = {10.1109/ICRA.2018.8461048}
}

% Verified

@article{choe2021fusion,
    author = {Jaesung Choe and Kyungdon Joo and Tooba Imtiaz and In So Kweon},
    title = {Volumetric Propagation Network: Stereo-LiDAR Fusion for Long-Range Depth Estimation},
    journal = {CoRR},
    volume = {abs/2103.12964},
    year = {2021},
    url = {https://arxiv.org/abs/2103.12964},
    eprinttype = {arXiv},
    eprint = {2103.12964},
    timestamp = {Tue, 06 Apr 2021 19:06:07 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-2103-12964.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Verified

@misc{yang2019fusion,
      title={Dense Depth Posterior (DDP) from Single Image and Sparse Range},
      author={Yanchao Yang and Alex Wong and Stefano Soatto},
      year={2019},
      eprint={1901.10034},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1901.10034},
}

% Verified

@INPROCEEDINGS{Cheng_2019_CVPR,
  author={Cheng, Xuelian and Zhong, Yiran and Dai, Yuchao and Ji, Pan and Li, Hongdong},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title={Noise-Aware Unsupervised Deep Lidar-Stereo Fusion},
  year={2019},
  volume={},
  number={},
  pages={6332-6341},
  keywords={Training;Feedback loop;Solid modeling;Laser radar;Three-dimensional displays;Sensor fusion;Sensors;Robotics + Driving;3D from Multiview and Sensors;RGBD sensors and analytics},
  doi={10.1109/CVPR.2019.00650}
}

% Verified

@misc{mai2021fusion,
      title={Sparse LiDAR and Stereo Fusion (SLS-Fusion) for Depth Estimationand 3D Object Detection},
      author={Nguyen Anh Minh Mai and Pierre Duthon and Louahdi Khoudour and Alain Crouzil and Sergio A. Velastin},
      year={2021},
      eprint={2103.03977},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.03977},
}

% Verified

@inproceedings{li2020fusion,
author = {Li, Chi and Cao, Zhiguo},
year = {2020},
month = {05},
pages = {11-15},
title = {LiDAR-Stereo: Dense Depth Estimation from Sparse LiDAR and Stereo Images},
doi = {10.1145/3404716.3404721}
}

@article{papoulis1975pg,
    author = {Papoulis, Athanasios},
    title = {A New Algorithm in Spectral Analysis and Band-Limited Extrapolation},
    journal = {IEEE Transactions on Circuits and Systems},
    volume = {22},
    number = {9},
    pages = {735--742},
    year = {1975}
}

@article{gerchberg1974pg,
    author = {Gerchberg, R. W.},
    title = {Super-Resolution through Error Energy Reduction},
    journal = {Optica Acta},
    volume = {21},
    number = {9},
    pages = {709--720},
    year = {1974}
}

@article{pgImageSuperRes2011,
    author = {Author, A. and Author, B.},
    title = {Application of the Papoulis--Gerchberg Method in Image Super-Resolution and Inpainting},
    journal = {International Journal of Image and Graphics},
    year = {2011},
    note = {Bu giriş, PG tabanlı bir inpainting/süper-çözünürlük makalesi için yer tutucudur; gerçek yazar ve dergi bilgilerini eklemelisin.}
}

% Verified

@article{uyanikDynamicPG,
    author = {Kuzucu Hıdır, Elvan and Öztürk, Dilan and Gül, Mustafa and Özbay, Bengisu and Arisoy, A and Sirin, H and Uyanık, İsmail},
    year = {2018},
    month = {03},
    pages = {014233121875989},
    title = {Enhancing 3D range image measurement density via dynamic Papoulis–Gerchberg algorithm},
    volume = {40},
    journal = {Transactions of the Institute of Measurement and Control},
    doi = {10.1177/0142331218759899}
}

% Verified

@misc{yin2021m2dgrmultisensormultiscenarioslam,
    title={M2DGR: A Multi-sensor and Multi-scenario SLAM Dataset for Ground Robots},
    author={Jie Yin and Ang Li and Tao Li and Wenxian Yu and Danping Zou},
    year={2021},
    eprint={2112.13659},
    archivePrefix={arXiv},
    primaryClass={cs.RO},
    url={https://arxiv.org/abs/2112.13659},
}

% Verified

@misc{citrus_farm_dataset,
    author = {UCR Robotics},
    title = {CitrusFarm Dataset},
    year = {2023},
    url = {https://ucr-robotics.github.io/Citrus-Farm-Dataset/},
    note = {Accessed: 2025-12-04}
}

% Verified

@article{pg-image-inpaint-2009,
    author = {Chatterjee, Priyam and Mukherjee, Sujata and Chaudhuri, Subhasis and Seetharaman, Guna},
    year = {2009},
    month = {01},
    pages = {80-89},
    title = {Application Of Papoulis-Gerchberg Method In Image Super-Resolution and Inpainting},
    volume = {52},
    journal = {Comput. J.},
    doi = {10.1093/comjnl/bxm050}
}

% Verified

@misc{grupp2017evo,
  author       = {Michael Grupp},
  title        = {evo: Python package for the evaluation of odometry and SLAM},
  year         = {2017},
  howpublished = {\url{https://github.com/MichaelGrupp/evo}},
  note         = {Accessed: 2025}
}

% Verified

@article{Kabsch:a12999,
author = "Kabsch, W.",
title = "{A solution for the best rotation to relate two sets of vectors}",
journal = "Acta Crystallographica Section A",
year = "1976",
volume = "32",
number = "5",
pages = "922--923",
month = "Sep",
doi = {10.1107/S0567739476001873},
url = {https://doi.org/10.1107/S0567739476001873},
abstract = {A simple procedure is derived which determines a best rotation of a given vector set into a second vector set by minimizing the weighted sum of squared deviations. The method is generalized for any given metric constraint on the transformation.},
}

% Verified

@inproceedings{teng2023multimodal,
  title={Multimodal Dataset for Localization, Mapping and Crop Monitoring in Citrus Tree Farms},
  author={Teng, Hanzhe and Wang, Yipeng and Song, Xiaoao and Karydis, Konstantinos},
  booktitle={International Symposium on Visual Computing},
  pages={571--582},
  year={2023}
}

% -----------------------------
% Dataset website: home
% -----------------------------
@misc{citrusfarm_website,
  author       = {{CitrusFarm Dataset Authors}},
  title        = {CitrusFarm Dataset (Project Website)},
  year         = {2023},
  howpublished = {Website},
  url          = {https://ucr-robotics.github.io/Citrus-Farm-Dataset/},
  urldate      = {2025-12-26}
}

% -----------------------------
% Dataset website: download page (ROS bag structure + topic list)
% -----------------------------
@misc{citrusfarm_download,
  author       = {{CitrusFarm Dataset Authors}},
  title        = {CitrusFarm Dataset: Download},
  year         = {2023},
  howpublished = {Website},
  url          = {https://ucr-robotics.github.io/Citrus-Farm-Dataset/download.html},
  urldate      = {2025-12-26},
  note         = {Contains folder structure, bag grouping, and ROS topic/message list.}
}

% -----------------------------
% Dataset website: calibration page (camera specs + 10 Hz statement)
% -----------------------------
@misc{citrusfarm_calib,
  author       = {{CitrusFarm Dataset Authors}},
  title        = {CitrusFarm Dataset: Calibration},
  year         = {2023},
  howpublished = {Website},
  url          = {https://ucr-robotics.github.io/Citrus-Farm-Dataset/calibration.html},
  urldate      = {2025-12-26},
  note         = {Contains sensor setup plus camera specifications (resolution, bit depth, FOV) and operating rates.}
}
